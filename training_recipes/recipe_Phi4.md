# ðŸ§  Phi-4 (14B)
### High-Quality, Reasoning-Centric Language Model

This document summarizes the **training recipe** for **Phi-4 (14B)** â€” Microsoftâ€™s reasoning-optimized large language model.  
Phi-4 builds on the Phi familyâ€™s design philosophy: **quality over quantity**, emphasizing synthetic data curation, curriculum learning, and long-context adaptation.

---

## ðŸ—ï¸ Model Architecture
- **Type:** Decoder-only Transformer, 14 billion parameters  
- **Tokenizer:** tiktoken (100 K vocabulary)  
- **Context length:** 4 K â†’ extended to **16 K** during mid-training  
- **Backbone:** Derived from Phi-3-medium with full attention (no sliding window)  
- **Training horizon:** â‰ˆ **10 trillion tokens**

---

## ðŸ§© 1. Pre-Training

### **Objective**
Develop a reasoning-oriented model with strong math, coding, and logic capabilities using a **hybrid synthetic + filtered organic** data mixture.

### **Data Composition**
- **Total tokens:** â‰ˆ **10 trillion**
- **Synthetic data (~40%)** generated through multiple workflows:
  - Seed curation from educational, scientific, and coding sources.
  - Iterative self-revision pipelines to improve correctness and style.
  - Task inversion to generate instructions from solutions.
  - Automated validation of code and math expressions.
- **Organic data (~60%)** includes filtered web, books, and code with high linguistic diversity.
  - Multi-stage LLM filtering (â‰ˆ 1M labels).
  - HTML/PDF/TeX extraction for equations and tables.
  - Decontamination against public benchmarks (7â€“13-gram match).

| Data Type | Fraction | Unique Tokens | Approx. Epochs |
|------------|-----------|----------------|----------------|
| Filtered Web | 15 % | 1.3 T | 1.2 Ã— |
| Web Rewrites (Synthetic) | 15 % | 0.29 T | 5.2 Ã— |
| Synthetic Reasoning / Instruction | 40 % | 0.29 T | 13.8 Ã— |
| Code (raw + synthetic) | 20 % | 0.82 T | 2.4 Ã— |
| Academic / Books | 10 % | 0.58 T | 1.7 Ã— |

> Total effective tokens â‰ˆ **10 T**, with heavier emphasis on synthetic repetition.

### **Optimization**
- Learning rate = 3e-4 (cosine decay with warm-up)
- Weight decay = 0.1  
- Global batch size â‰ˆ 5.8K tokens Ã— GPU shards  
- Stable optimization tuned via short-horizon pilot runs

---

## ðŸ§© 2. Mid-Training (Long-Context Adaptation)

### **Goal**
Expand context window (4 K â†’ 16 K) and strengthen multi-document reasoning.

### **Implementation**
- Continue training from the pre-trained checkpoint for â‰ˆ **250 B tokens**
- **30 %** long-context samples (> 8 K tokens), mostly from academic and code data
- **70 %** reused recall data from pre-training
- Extended synthetic data via concatenation and padding
- **RoPE base frequency = 250 K**
- Learning rate reduced by 10Ã—

---

## ðŸ§© 3. Post-Training

Transform Phi-4 into a **safe, instruction-following assistant** through three stages.

### **Stage 1 â€“ Supervised Fine-Tuning (SFT)**
- **Tokens:** â‰ˆ 8 billion (ChatML format)
- **Learning rate:** 1e-6
- **Domains:** math, coding, reasoning, safety, multilingual (â‰ˆ 40 languages)
- **Goal:** teach structured instruction-following and safe refusal.

### **Stage 2 â€“ Direct Preference Optimization (DPO)**
Two rounds applied to the SFT model:

1. **Pivotal Token DPO (â‰ˆ 300 K pairs)**  
   - Targets token-level corrections for reasoning and code tasks.  
   - Pairs identified using pivotal token search.

2. **Judge-Guided DPO (â‰ˆ 850 K pairs)**  
   - GPT-4o-based judges label positive vs. negative responses.  
   - Scoring dimensions: accuracy, completeness, style.

### **Stage 3 â€“ Hallucination Mitigation**
- Trains explicit **refusal behavior** for low-confidence or nonsensical inputs.
- Synthetic â€œbogus questionâ€ datasets encourage safe decline responses.
- Achieves ~80% reduction in hallucinations on SimpleQA tasks.

---

## ðŸ“Š Scale Summary

| Stage | Token Count / Samples | Purpose |
|--------|----------------------|----------|
| **Pre-training** | â‰ˆ 10 T tokens (4 K ctx) | Foundational reasoning |
| **Mid-training** | â‰ˆ 0.25 T tokens (16 K ctx) | Long-context adaptation |
| **SFT** | â‰ˆ 8 B tokens | Instruction & safety |
| **DPO (2 rounds)** | â‰ˆ 1.1 M pairs | Preference alignment & refinement |

---

## âš™ï¸ Key Insights

- **Synthetic data dominates:** high-quality synthetic reasoning drives the modelâ€™s logic performance.  
- **Curriculum learning:** data mixture gradually transitions from natural web â†’ structured reasoning â†’ supervised refinement.  
- **Mid-training as context curriculum:** progressively longer samples improve long-context stability.  
- **Token-level alignment:** pivotal-token DPO adds fine-grained reward signal.  
- **Refusal training:** improves safety and factual restraint.  
- **Total data exposure:** â‰ˆ **10.3 trillion tokens** (pre + mid + post).

---

## ðŸ“š Citation

> Microsoft Research.  
> **Phi-4 Technical Report: Scaling Small Language Models with High-Quality Synthetic Data.**  
> *arXiv preprint arXiv:2412.08905 (2024).*
