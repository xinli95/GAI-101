# ğŸ§  Qwen 3

### Third-Generation Reasoning-Centric **Text** LLMs by Alibaba Group (2025)

> Scope note: **Qwen 3 is a text LLM family** (dense + MoE, 0.6Bâ€“235B). Vision/Audio models are separate lines and not part of this technical report.

---

## ğŸš€ Overview

Qwen 3 introduces a unified pretraining â†’ post-training recipe emphasizing **highâ€‘quality synthetic reasoning**, **longâ€‘context training**, and **controllable thinking**. It also provides an efficient **strongâ€‘toâ€‘weak distillation** route for small models.

---

## ğŸ—ï¸ Model Architecture (at a glance)

- **Backbone:** Decoderâ€‘only Transformer (dense & MoE variants)
- **Tokenizer:** ~151K vocabulary
- **Context Length:** 32K base, extendable to **128K** via longâ€‘context training (YaRN)
- **Attention:** Grouped Query Attention (GQA)
- **Norm/Act:** RMSNorm, SwiGLU
- **Training precision:** bfloat16, ZeROâ€‘3

---

## ğŸ§© Pretraining

- **Objective:** Causal LM (nextâ€‘token prediction)
- **Scale:** ~**18T tokens**
- **Mixture:** filtered web/books/conversations; **code**; **math/science**; **multilingual** (>40 languages); **synthetic reasoning** (CoT)
- **Data QA:** LLMâ€‘based quality filtering, dedup, benchmark decontamination
- **Curriculum:** 3 stages with growing max sequence length and reasoning intensity
  1. ~6T @ 4K â†’ core language
  2. ~8T @ 8Kâ€“16K â†’ reasoning + context expansion
  3. ~4T @ 32K â†’ longâ€‘context + code

**Longâ€‘context method:** **YaRN** (RoPE enlargement) + training on concatenated long documents; objective remains NTP.

---

## ğŸ“ Postâ€‘Training Pipeline (4 stages)

1. **Longâ€‘CoT Cold Start**  
   Largeâ€‘scale synthetic CoT traces with verified answers to seed structured reasoning.

2. **Reasoning RL**  
   Verifiableâ€‘reward RL (math/coding/logic) with stability tricks (e.g., temperature annealing, reward normalization).

3. **Thinkingâ€‘Mode Fusion**  
   Unite _thinking_ and _nonâ€‘thinking_ behaviors in one chat model.

   - Prompt/API switches (e.g., `/think`, `/no think`, optional `<think>â€¦</think>` blocks).
   - Fusion SFT set = **thinking** samples (via rejection sampling from the reasoning model) + **nonâ€‘thinking** samples (instruction, coding, multilingual, translation, roleâ€‘play, QA).
   - Improves instruction adherence and enables robust **mode switching**.

4. **General RL**  
   Preferenceâ€‘based tuning (hybrid DPOâ€‘RLHF) for instruction following, tone, safety, and factuality while preserving reasoning.

---

## â±ï¸ Thinking Budget (inferenceâ€‘time control)

Allocate a **budget of â€œthinking tokensâ€** per request. Higher budgets â†’ stronger reasoning (more steps); lower budgets â†’ lower latency. This leverages the fused mode interface to make reasoning depth **userâ€‘controllable**.

---

## ğŸ§¬ Strongâ€‘toâ€‘Weak Distillation (order clarified)

**Goal:** Train smaller **students** (0.6Bâ€“8B) efficiently under large **teacher** models without reâ€‘running the full RL pipeline.

**Twoâ€‘phase order (correct):**

1. **Offâ€‘policy distillation (first):**

   - Train the student on **fixed teacher outputs/logits** (combined thinking + nonâ€‘thinking responses).
   - Transfers broad competence and formatting quickly.

2. **Onâ€‘policy distillation (second):**
   - Let the **student** roll out on prompts; **align** its behavior to the teacherâ€™s via KL/logit matching on the studentâ€™s own trajectories.
   - Reduces distribution mismatch and sharpens reasoning.

**Why this path:** Comparable or better quality than RL with **~10Ã— less compute**, producing small models that retain the teacherâ€™s reasoning and alignment behavior.

---

## âš™ï¸ Key Takeaways

- **Textâ€‘only LLM family:** Qwen 3 focuses on language models; multimodal lines are separate.
- **Thinkingâ€‘Mode Fusion + Budget:** Practical, controllable reasoning depth per query.
- **Efficient scaling:** Strongâ€‘toâ€‘weak distillation (offâ€‘policy â†’ onâ€‘policy) yields capable small models.
- **Data quality > data volume:** Heavy use of filtered and synthetic reasoning data.
- **Curriculum + YaRN:** Smooth extension to 128K context for multiâ€‘document tasks.

---

## ğŸ“š Citation

Qwen Team. â€œ**Qwen3 Technical Report**â€ arXiv:2505.09388 (2025).
